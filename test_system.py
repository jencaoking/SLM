# -*- coding: utf-8 -*-
"""
é—®ç­”ç³»ç»Ÿç»¼åˆæµ‹è¯•è„šæœ¬

æµ‹è¯•æ•´ä¸ªç³»ç»Ÿçš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒã€æ¨ç†ç­‰ã€‚
"""

import os
import sys
import json
import logging
import traceback
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
modules_available = {}
try:
    from utils.format_detector import FormatDetector
    modules_available['format_detector'] = True
except ImportError as e:
    print(f"å¯¼å…¥FormatDetectorå¤±è´¥: {e}")
    modules_available['format_detector'] = False

try:
    from utils.data_loader import UniversalDataLoader
    modules_available['data_loader'] = True
except ImportError as e:
    print(f"å¯¼å…¥UniversalDataLoaderå¤±è´¥: {e}")
    modules_available['data_loader'] = False

try:
    from utils.data_processor import DataProcessor
    modules_available['data_processor'] = True
except ImportError as e:
    print(f"å¯¼å…¥DataProcessorå¤±è´¥: {e}")
    modules_available['data_processor'] = False

try:
    from utils.tokenizer import QATokenizer
    modules_available['tokenizer'] = True
except ImportError as e:
    print(f"å¯¼å…¥QATokenizerå¤±è´¥: {e}")
    modules_available['tokenizer'] = False

try:
    from utils.metrics import QAMetrics
    modules_available['metrics'] = True
except ImportError as e:
    print(f"å¯¼å…¥QAMetricså¤±è´¥: {e}")
    modules_available['metrics'] = False

try:
    from models.transformer import QATransformer
    modules_available['transformer'] = True
except ImportError as e:
    print(f"å¯¼å…¥QATransformerå¤±è´¥: {e}")
    modules_available['transformer'] = False

try:
    from data_validator import DataValidator
    modules_available['data_validator'] = True
except ImportError as e:
    print(f"å¯¼å…¥DataValidatorå¤±è´¥: {e}")
    modules_available['data_validator'] = False


class SystemTester:
    """ç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self, log_level: str = "INFO"):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.setup_logging(log_level)
        self.logger = logging.getLogger(__name__)
        
        self.test_results = {}
        self.failed_tests = []
        
        # æµ‹è¯•æ•°æ®è·¯å¾„
        self.data_dir = Path("data/raw/custom")
        self.test_files = [
            self.data_dir / "qa_data.csv",
            self.data_dir / "qa_data.json", 
            self.data_dir / "qa_data.jsonl"
        ]
        
    def setup_logging(self, level: str):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=getattr(logging, level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout)
            ]
        )
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.logger.info("å¼€å§‹ç³»ç»Ÿç»¼åˆæµ‹è¯•...")
        
        tests = []
        
        # åªæ·»åŠ å¯ç”¨æ¨¡å—çš„æµ‹è¯•
        if modules_available.get('format_detector'):
            tests.append(("æ•°æ®æ ¼å¼æ£€æµ‹", self.test_format_detection))
        
        if modules_available.get('data_loader'):
            tests.append(("æ•°æ®åŠ è½½åŠŸèƒ½", self.test_data_loading))
        
        if modules_available.get('data_validator'):
            tests.append(("æ•°æ®éªŒè¯åŠŸèƒ½", self.test_data_validation))
        
        if modules_available.get('tokenizer'):
            tests.append(("åˆ†è¯å™¨åŠŸèƒ½", self.test_tokenizer))
        
        if modules_available.get('data_processor'):
            tests.append(("æ•°æ®å¤„ç†åŠŸèƒ½", self.test_data_processing))
        
        if modules_available.get('transformer'):
            tests.append(("æ¨¡å‹åˆ›å»º", self.test_model_creation))
        
        if modules_available.get('metrics'):
            tests.append(("è¯„ä¼°æŒ‡æ ‡", self.test_metrics))
        
        # é…ç½®åŠ è½½æµ‹è¯•ä¸ä¾èµ–ç‰¹æ®Šæ¨¡å—
        tests.append(("é…ç½®åŠ è½½", self.test_config_loading))
        tests.append(("æ–‡ä»¶ç»“æ„æ£€æŸ¥", self.test_file_structure))
        
        for test_name, test_func in tests:
            self.logger.info(f"è¿è¡Œæµ‹è¯•: {test_name}")
            try:
                result = test_func()
                self.test_results[test_name] = {
                    'status': 'passed' if result else 'failed',
                    'details': result if isinstance(result, dict) else {}
                }
                
                if result:
                    self.logger.info(f"âœ… {test_name} - é€šè¿‡")
                else:
                    self.logger.error(f"âŒ {test_name} - å¤±è´¥")
                    self.failed_tests.append(test_name)
                    
            except Exception as e:
                self.logger.error(f"âŒ {test_name} - å¼‚å¸¸: {str(e)}")
                self.test_results[test_name] = {
                    'status': 'error',
                    'error': str(e),
                    'traceback': traceback.format_exc()
                }
                self.failed_tests.append(test_name)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_test_report()
        
        return self.test_results
    
    def test_format_detection(self) -> bool:
        """æµ‹è¯•æ ¼å¼æ£€æµ‹åŠŸèƒ½"""
        if not modules_available.get('format_detector'):
            self.logger.warning("FormatDetectoræ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return True
            
        try:
            detector = FormatDetector()
            
            for test_file in self.test_files:
                if not test_file.exists():
                    self.logger.warning(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
                    continue
                
                result = detector.detect_format(test_file)
                
                if not result['is_valid']:
                    self.logger.error(f"æ ¼å¼æ£€æµ‹å¤±è´¥: {test_file}")
                    return False
                
                expected_formats = {
                    'qa_data.csv': 'csv',
                    'qa_data.json': 'json',
                    'qa_data.jsonl': 'jsonl'
                }
                
                expected_format = expected_formats.get(test_file.name)
                if expected_format and result['format'] != expected_format:
                    self.logger.error(f"æ ¼å¼æ£€æµ‹é”™è¯¯: {test_file}, æœŸæœ›: {expected_format}, å®é™…: {result['format']}")
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ ¼å¼æ£€æµ‹æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_data_loading(self) -> bool:
        """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
        if not modules_available.get('data_loader'):
            self.logger.warning("UniversalDataLoaderæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return True
            
        try:
            for test_file in self.test_files:
                if not test_file.exists():
                    continue
                
                loader = UniversalDataLoader(data_path=test_file)
                data = loader.load_data()
                
                if not data:
                    self.logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {test_file}")
                    return False
                
                # æ£€æŸ¥æ•°æ®ç»“æ„
                required_fields = ['question', 'context', 'answer']
                for record in data[:5]:  # æ£€æŸ¥å‰5æ¡è®°å½•
                    for field in required_fields:
                        if field not in record:
                            self.logger.error(f"ç¼ºå°‘å¿…éœ€å­—æ®µ {field}: {test_file}")
                            return False
                
                self.logger.info(f"æ•°æ®åŠ è½½æˆåŠŸ: {test_file}, è®°å½•æ•°: {len(data)}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åŠ è½½æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_data_validation(self) -> bool:
        """æµ‹è¯•æ•°æ®éªŒè¯åŠŸèƒ½"""
        if not modules_available.get('data_validator'):
            self.logger.warning("DataValidatoræ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return True
            
        try:
            validator = DataValidator(log_level="ERROR")  # å‡å°‘æ—¥å¿—è¾“å‡º
            
            for test_file in self.test_files:
                if not test_file.exists():
                    continue
                
                result = validator.validate_file(str(test_file))
                
                if result['overall_status'] in ['failed', 'error']:
                    self.logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {test_file}")
                    return False
                
                self.logger.info(f"æ•°æ®éªŒè¯é€šè¿‡: {test_file}, çŠ¶æ€: {result['overall_status']}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ•°æ®éªŒè¯æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_tokenizer(self) -> bool:
        """æµ‹è¯•åˆ†è¯å™¨åŠŸèƒ½"""
        if not modules_available.get('tokenizer'):
            self.logger.warning("QATokenizeræ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return True
            
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_texts = [
                "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                "Machine learning is great!",
                "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿ç”¨statistical techniquesã€‚"
            ]
            
            # åˆ›å»ºåˆ†è¯å™¨
            tokenizer = QATokenizer(vocab_size=1000, language="mixed")
            
            # æ„å»ºè¯æ±‡è¡¨
            tokenizer.build_vocab(test_texts)
            
            # æµ‹è¯•ç¼–ç å’Œè§£ç 
            for text in test_texts:
                encoded = tokenizer.encode(text)
                decoded = tokenizer.decode(encoded['input_ids'])
                
                if not encoded['input_ids']:
                    self.logger.error(f"ç¼–ç å¤±è´¥: {text}")
                    return False
                
                if not decoded:
                    self.logger.error(f"è§£ç å¤±è´¥: {text}")
                    return False
            
            self.logger.info(f"åˆ†è¯å™¨æµ‹è¯•é€šè¿‡ï¼Œè¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆ†è¯å™¨æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_data_processing(self) -> bool:
        """æµ‹è¯•æ•°æ®å¤„ç†åŠŸèƒ½"""
        if not modules_available.get('data_processor') or not modules_available.get('data_loader') or not modules_available.get('tokenizer'):
            self.logger.warning("æ•°æ®å¤„ç†ç›¸å…³æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return True
            
        try:
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_file = self.test_files[0]  # ä½¿ç”¨CSVæ–‡ä»¶
            if not test_file.exists():
                self.logger.warning("æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®å¤„ç†æµ‹è¯•")
                return True
            
            loader = UniversalDataLoader(data_path=test_file)
            data = loader.load_data()
            
            # åˆ›å»ºåˆ†è¯å™¨
            all_texts = []
            for record in data:
                all_texts.extend([record.get('question', ''), record.get('context', ''), record.get('answer', '')])
            
            tokenizer = QATokenizer(vocab_size=1000)
            tokenizer.build_vocab(all_texts)
            
            # åˆ›å»ºæ•°æ®å¤„ç†å™¨
            processor = DataProcessor(tokenizer=tokenizer, max_length=256)
            
            # å¤„ç†æ•°æ®
            processed_data = processor.process_data(data[:5], is_training=True)  # åªå¤„ç†å‰5æ¡
            
            if not processed_data:
                self.logger.error("æ•°æ®å¤„ç†å¤±è´¥")
                return False
            
            self.logger.info(f"æ•°æ®å¤„ç†æˆåŠŸï¼Œå¤„ç†å‰: {len(data[:5])}, å¤„ç†å: {len(processed_data)}")
            return True
            
        except Exception as e:
            self.logger.error(f"æ•°æ®å¤„ç†æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_model_creation(self) -> bool:
        """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
        if not modules_available.get('transformer'):
            self.logger.warning("QATransformeræ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return True
            
        try:
            # åˆ›å»ºä¸€ä¸ªå°å‹æ¨¡å‹ç”¨äºæµ‹è¯•
            model = QATransformer(
                vocab_size=1000,
                d_model=128,
                n_heads=4,
                n_encoder_layers=2,
                d_ff=256,
                max_position_embeddings=128,
                dropout=0.1
            )
            
            # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
            batch_size = 2
            seq_len = 32
            
            question_ids = torch.randint(1, 1000, (batch_size, seq_len))
            context_ids = torch.randint(1, 1000, (batch_size, seq_len))
            
            try:
                import torch
                outputs = model(
                    question_input_ids=question_ids,
                    context_input_ids=context_ids
                )
                
                if 'start_logits' not in outputs or 'end_logits' not in outputs:
                    self.logger.error("æ¨¡å‹è¾“å‡ºæ ¼å¼é”™è¯¯")
                    return False
                
                self.logger.info(f"æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
                return True
                
            except ImportError:
                self.logger.warning("PyTorchæœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹æµ‹è¯•")
                return True
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åˆ›å»ºæµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_metrics(self) -> bool:
        """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡"""
        if not modules_available.get('metrics'):
            self.logger.warning("QAMetricsæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
            return True
            
        try:
            metrics = QAMetrics()
            
            # æµ‹è¯•æ•°æ®
            predictions = ["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ç½‘ç»œ", "è‡ªç„¶è¯­è¨€å¤„ç†"]
            references = ["æœºå™¨å­¦ä¹ ç®—æ³•", "æ·±åº¦ç¥ç»ç½‘ç»œ", "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯"]
            
            # æµ‹è¯•å„é¡¹æŒ‡æ ‡
            em = metrics.compute_exact_match(predictions, references)
            f1 = metrics.compute_f1_score(predictions, references)
            bleu = metrics.compute_bleu_score(predictions, references)
            rouge = metrics.compute_rouge_score(predictions, references)
            
            if not (0 <= em <= 1 and 0 <= f1 <= 1):
                self.logger.error("è¯„ä¼°æŒ‡æ ‡å€¼å¼‚å¸¸")
                return False
            
            self.logger.info(f"è¯„ä¼°æŒ‡æ ‡æµ‹è¯•é€šè¿‡ - EM: {em:.4f}, F1: {f1:.4f}")
            return True
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°æŒ‡æ ‡æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_config_loading(self) -> bool:
        """æµ‹è¯•é…ç½®åŠ è½½"""
        try:
            import yaml
            
            config_file = Path("configs/config.yaml")
            if not config_file.exists():
                self.logger.warning("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
                return True
            
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            required_sections = ['model', 'training', 'data', 'paths']
            for section in required_sections:
                if section not in config:
                    self.logger.error(f"é…ç½®æ–‡ä»¶ç¼ºå°‘èŠ‚: {section}")
                    return False
            
            self.logger.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"é…ç½®åŠ è½½æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_file_structure(self) -> bool:
        """æµ‹è¯•æ–‡ä»¶ç»“æ„"""
        try:
            required_dirs = [
                Path("configs"),
                Path("models"), 
                Path("utils"),
                Path("data/raw/custom")
            ]
            
            for dir_path in required_dirs:
                if not dir_path.exists():
                    self.logger.error(f"ç¼ºå°‘ç›®å½•: {dir_path}")
                    return False
            
            required_files = [
                Path("configs/config.yaml"),
                Path("models/__init__.py"),
                Path("utils/__init__.py"),
                Path("train.py"),
                Path("inference.py"),
                Path("requirements.txt")
            ]
            
            for file_path in required_files:
                if not file_path.exists():
                    self.logger.error(f"ç¼ºå°‘æ–‡ä»¶: {file_path}")
                    return False
            
            self.logger.info("æ–‡ä»¶ç»“æ„æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ç»“æ„æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result['status'] == 'passed')
        failed_tests = len(self.failed_tests)
        
        self.logger.info("\n" + "="*60)
        self.logger.info("æµ‹è¯•æŠ¥å‘Š")
        self.logger.info("="*60)
        self.logger.info(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        self.logger.info(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
        self.logger.info(f"å¤±è´¥æµ‹è¯•: {failed_tests}")
        self.logger.info(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
        
        if self.failed_tests:
            self.logger.info(f"\nå¤±è´¥çš„æµ‹è¯•:")
            for test_name in self.failed_tests:
                self.logger.info(f"  - {test_name}")
        
        self.logger.info("="*60)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = Path("outputs/test_report.json")
        report_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'total_tests': total_tests,
                    'passed_tests': passed_tests,
                    'failed_tests': failed_tests,
                    'success_rate': passed_tests/total_tests*100
                },
                'details': self.test_results,
                'failed_tests': self.failed_tests
            }, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("é—®ç­”ç³»ç»Ÿç»¼åˆæµ‹è¯•")
    print("="*50)
    
    # æ£€æŸ¥å¿…è¦çš„ä¾èµ–
    try:
        import torch
        print("âœ… PyTorch å·²å®‰è£…")
    except ImportError:
        print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œå°†è·³è¿‡æ¨¡å‹ç›¸å…³æµ‹è¯•")
    
    try:
        import yaml
        print("âœ… PyYAML å·²å®‰è£…")
    except ImportError:
        print("âŒ PyYAML æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install PyYAML")
        return
    
    try:
        import pandas
        print("âœ… Pandas å·²å®‰è£…")
    except ImportError:
        print("âŒ Pandas æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install pandas")
        return
    
    print("\nå¼€å§‹ç³»ç»Ÿæµ‹è¯•...")
    
    # è¿è¡Œæµ‹è¯•
    tester = SystemTester()
    results = tester.run_all_tests()
    
    # è¿”å›é€€å‡ºç 
    failed_count = len(tester.failed_tests)
    if failed_count == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print(f"\nâŒ {failed_count} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())